# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5 выполнил:
- Бердышев Артём Александрович
- РИ210942
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

## Цель работы
Обучить ML-Agent'a и научиться совместно его использовать с проектом Unity в экономической системе

## Задание 1
### Измените параметры файла. yaml-агента и определить какие параметры и как влияют на обучение модели.

Скачиваем проект, открываем его в Unity, устанавливаем ML-Agenta

![изображение](https://user-images.githubusercontent.com/104849066/205010335-29f1be5b-132f-4a15-ba63-e59f5d24fd60.png)

Перед тем как перейти к началу обучения, запустим Anaconda Prompt (от имени администратора) и создадим виртуальное пространтсво
```conda create -n MLAgent python=3.6.13```

И активируем его ```conda activate MLAgent```

Теперь установим необходимые для работы пакеты:
```
pip install mlagents==0.28.0
pip install torch~=1.7.1 --f https://download.pytorch.org/whl/torch_stable.html
```
Запускаем нашего ML-Agent'a на обучение: ```mlagents-learn Economic.yaml --run-id=Economic --force``` 

![изображение](https://user-images.githubusercontent.com/104849066/205305801-cc53ec3e-1fb9-445d-b84e-5ec84c603faa.png)

Наблюдаем за обучением модели...

![изображение](https://user-images.githubusercontent.com/104849066/205306231-ae673120-b5fb-47d1-9ed0-448036ea8d45.png)

Спустя некоторое время завершаем обучение модели

![изображение](https://user-images.githubusercontent.com/104849066/205306895-1f8ba89e-8d50-4bd1-8ff5-e1f053be5441.png)

Далее построим графики для оценки результатов обучения. Для этого установливаем TensorBoard ```pip install tensorflow```

TensorBoard успешно установился

![изображение](https://user-images.githubusercontent.com/104849066/205307709-580db132-416e-487b-82b5-d777a3ab8dcb.png)

Теперь запустим TensorBoard ```tensorboard --logdir=results```

Зайдём на сайт TensorBoard и посмотрим стандатные графики

![изображение](https://user-images.githubusercontent.com/104849066/205310084-0e92c636-b387-463a-bb40-129914c8ac96.png)

![изображение](https://user-images.githubusercontent.com/104849066/205310155-2079aadb-57c2-4c9a-9ba0-ef3b399ffcfc.png)

![изображение](https://user-images.githubusercontent.com/104849066/205310198-8e3f3060-173a-42ac-aca7-103ff4b43749.png)

## Задание 2
### Измените параметры файла. yaml-агента и определить какие параметры и как влияют на обучение модели.Опишите результаты, выведенные в TensorBoard.

Попробуем поменять параметры ```batch_size``` и ```learning_rate```. Отвечающие за количество опытов на каждой итерации градиентного спуска и за начальную скорость обучения

![изображение](https://user-images.githubusercontent.com/104849066/205314984-e480a0cc-06d0-4bdf-a56c-b95957869054.png)

Запускаем обучение всё той же командой
``` mlagents-learn Economic.yaml --run-id=Economic --force ```

### Спустя 40000 шагов останавливаем обучение и смотрим на результат

**Увеличилось** воздействие окружающей среды, значение **Cumulative Reward** было **0.576** стало **0.8**

![изображение](https://user-images.githubusercontent.com/104849066/205317909-5a3914e7-93e3-4b33-a3ec-1eccf5bb3a2c.png)

Графики, отвечающие за **Потери**, говорят, что **Value Loss(значения потерь)** **увеличилось**: было **3.355** стало **7.584**

![изображение](https://user-images.githubusercontent.com/104849066/205318792-df6569e8-3e69-4e97-b38b-e5995f2e5dc2.png)

В **Policy** изменились только последние 3 графика. **Увеличилось** значение графиков **(внешняя награда) Extrinsic Reward(было: 0.57 стало: 0.82)** , **(Оценка внешней стоимости) Extrinsic Value Estimate(было: -11.9 стало: -5.3)** и **(скорость обучения)Learning Rate(было: 2.9е-4 стало: 9е-4)**

![изображение](https://user-images.githubusercontent.com/104849066/205321282-7bd598a1-d966-4cc4-bd2d-8e08f3556ee9.png)

### Судя по данным на графиках, можно сказать, что эти изменения отрицательно сказались на обучении модели, т.к значения потерь увеличилось почти в 2 раза

### Вернём параметры в исходное состояние и попробуем поменять другие

На этот раз изменим параметры ```beta```, ```epsilon```, ```num_epoch```, ```num_layers```, ```gamma``` следующим образом:

![изображение](https://user-images.githubusercontent.com/104849066/205324569-ada38666-7c3f-4ecd-9c12-3b821fb5bc79.png)

### После 40000 шагов останавливаем обучение и идём смотреть результаты

**Увеличилось** воздействие окружающей среды, значение **Cumulative Reward** было **0.576** стало **1.0**

![изображение](https://user-images.githubusercontent.com/104849066/205326155-c9fe56c4-55c9-4934-8eef-4648892a1903.png)

В **Losses** говорят, что **Value Loss(значения потерь)** **уменьшилось**: было **3.355** стало **2.131**

![изображение](https://user-images.githubusercontent.com/104849066/205326499-1af4eaa0-e3dc-408e-a239-a883a3265038.png)

В **Policy** Изменились значения всех графиков, а именно:

**Увеличилось** значение у **Epsilon(было: 0.197 стало: 0.3), у ** Внешней Награды (Extrinsic Reward)(было: 0.57 стало: 1.0)** и у **Extrinsic Value Estimate(было: -11.9 стало: -0.8)** 

**Уменьшилось** значение у **Beta(было: 9.68е-3 стало: 5е-3)**, y **Entropy(было: 1.417 стало: 1.414)**

![изображение](https://user-images.githubusercontent.com/104849066/205326737-5c67deee-1991-4508-9bbf-fa09f7f76c6a.png)

### А вот эти изменения положительно сказались на обучении модели, ведь значение потерь стали меньше на 33%

### Выводы
В ходе этой лабораторной работы я научился использовать ML-Agent'a в связке с экономической системой в Unity. Таким образом можно бороться с инфляцией в игре. Мне было интересно менять значения в файле конфигурации и смотреть, как они влияют на результаты обучения агента. Визуализировать этот процесс помогает TensorBoard, которой я так же научился пользоваться в ходе выполнения лабораторной работы

## Powered by

**Artem Berdyshev**
